{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allanz/miniconda3/envs/nanodo/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-24 23:59:13.849197: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742885953.865973 2353299 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742885953.871400 2353299 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742885953.884859 2353299 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742885953.884874 2353299 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742885953.884875 2353299 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742885953.884877 2353299 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "from nanodo.model_factory import *\n",
    "from nanodo.data import *\n",
    "from nanodo.configs.default import *\n",
    "from nanodo.train import *\n",
    "import numpy as np\n",
    "import orbax.checkpoint as ocp\n",
    "from orbax.checkpoint import PyTreeCheckpointer\n",
    "from flax.core import unfreeze\n",
    "import jax.numpy as jnp\n",
    "from nanodo.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allanz/miniconda3/envs/nanodo/lib/python3.11/site-packages/orbax/checkpoint/_src/serialization/type_handlers.py:1250: UserWarning: Couldn't find sharding info under RestoreArgs. Populating sharding info from sharding file. Please note restoration time will be slightly increased due to reading from file instead of directly from RestoreArgs. Note also that this option is unsafe when restoring on a different topology than the checkpoint was saved with.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "c = get_config()\n",
    "#checkpoint = \"/home/allanz/nanodo_workdir/92000.orbax-checkpoint-tmp-138\"\n",
    "checkpoint = \"/home/allanz/nanodo_workdir/90000/state\"\n",
    "params= PyTreeCheckpointer().restore(checkpoint)\n",
    "test = unfreeze(params['params'])\n",
    "params = params['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_config = ml_collections.config_dict.create(\n",
    "      D=512,  # model/embed dim  = qkv dim\n",
    "      F=2048,  # FF inner dimension\n",
    "      H=8,  # num attention heads\n",
    "      L=128,  # max context/sequence length (move out of config?)\n",
    "      N=6,  # number of transformer block layers\n",
    "      dtype=\"float32\",  # computation dtype.\n",
    "      fsdp_enabled=True,  # True to shard the model.\n",
    "      remat=False,  # Transformer block gradient checkpointing to save memory.\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerDo(\n",
      "    # attributes\n",
      "    docfg = DoConfig(D=512, H=8, L=128, N=6, V=32101, F=2048, kernel_init=<function variance_scaling.<locals>.init at 0x7f2cf1e207c0>, embed_init=<function variance_scaling.<locals>.init at 0x7f2cf1e20900>, dtype='float32', fsdp_enabled=True, remat=False)\n",
      ")\n",
      "TransformerDo(\n",
      "    # attributes\n",
      "    docfg = DoConfig(D=512, H=8, L=128, N=6, V=32101, F=2048, kernel_init=<function variance_scaling.<locals>.init at 0x7f2cf1e207c0>, embed_init=<function variance_scaling.<locals>.init at 0x7f2cf1e20900>, dtype='bfloat16', fsdp_enabled=True, remat=False)\n",
      ")\n",
      "TransformerDo(\n",
      "    # attributes\n",
      "    docfg = DoConfig(D=512, H=8, L=128, N=6, V=32101, F=2048, kernel_init=<function variance_scaling.<locals>.init at 0x7f2cf1e207c0>, embed_init=<function variance_scaling.<locals>.init at 0x7f2cf1e20900>, dtype=<class 'jax.numpy.float32'>, fsdp_enabled=True, remat=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_py_tokenizer(\"tests/testdata/sentencepiece_cc_all.32000.100extra-sentencepiece.model\")\n",
    "vocab_size = tokenizer.GetPieceSize()\n",
    "cfg = DoConfig(**test_config, V=vocab_size)  # pytype:disable=attribute-error\n",
    "# model without fsdp\n",
    "module = TransformerDo(cfg) \n",
    "print(module)\n",
    "# model with fsdp\n",
    "transformer, _ = get_model_and_loss(c, vocab_size)\n",
    "print(transformer)\n",
    "# same model but with dtype as jax.numpy.float32\n",
    "docfg = model.DoConfig(D=512, H=8, L=128, N=6, V=vocab_size, F=2048)\n",
    "m = model.TransformerDo(docfg)\n",
    "print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(42)\n",
    "_, init_rng = jax.random.split(rng)\n",
    "x = jnp.ones((8, 512), dtype=jnp.int32)\n",
    "initial_variables = jax.jit(m.init)(init_rng, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:absl:Applying deprecated PyGrain MapOperation. Please use the grain.python.MapTransform.\n"
     ]
    }
   ],
   "source": [
    "test_set = py_batched_tfds(\n",
    "          tfds_name=\"c4_10k\",\n",
    "          split=\"train\",\n",
    "          context_size=512,\n",
    "          worker_count=0,\n",
    "          vocab_path=\"tests/testdata/sentencepiece_cc_all.32000.100extra-sentencepiece.model\",\n",
    "          batch_size = 8\n",
    "          )\n",
    "\n",
    "batch = next(iter(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.linen import Partitioned\n",
    "\n",
    "def make_partitioned(array, names):\n",
    "    partition_array = Partitioned(array, names = names, mesh = None)\n",
    "    return partition_array\n",
    "\n",
    "def convert_attn_blocks(params):\n",
    "    blocks = [\"blocks_0\", \"blocks_1\", \"blocks_2\", \"blocks_3\", \"blocks_4\", \"blocks_5\"]\n",
    "    switches = {\"attn_out_proj\": (None, None, 'data'), \"key\": ('data', None), \"query\":('data', None), \"value\":('data', None)}\n",
    "\n",
    "    for block in blocks:\n",
    "          for switch in switches: \n",
    "               #print(params[block][\"CausalAttn_0\"][switch][\"kernel\"])\n",
    "               params[block][\"CausalAttn_0\"][switch][\"kernel\"] = make_partitioned(params[block][\"CausalAttn_0\"][switch][\"kernel\"][\"value\"], switches[switch])\n",
    "\n",
    "def convert_Mlp(params):\n",
    "    blocks = [\"blocks_0\", \"blocks_1\", \"blocks_2\", \"blocks_3\", \"blocks_4\", \"blocks_5\"]\n",
    "    switches = {\"Dense_0\": ('data', None), \"Dense_1\": ('data', None)}\n",
    "\n",
    "    for block in blocks:\n",
    "          for switch in switches: \n",
    "               #print(params[block][\"Mlp_0\"][switch][\"kernel\"])\n",
    "               params[block][\"Mlp_0\"][switch][\"kernel\"] = make_partitioned(params[block][\"Mlp_0\"][switch][\"kernel\"][\"value\"], switches[switch])\n",
    "\n",
    "\n",
    "def convert_embed(params):\n",
    "    params[\"embed\"][\"embedding\"] = make_partitioned(params[\"embed\"][\"embedding\"][\"value\"], (None, 'data'))\n",
    "\n",
    "def convert_pos_embed(params):\n",
    "    params[\"pos_embed\"][\"embedding\"] = make_partitioned(params[\"pos_embed\"][\"embedding\"][\"value\"], (None, 'data'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_attn_blocks(params)\n",
    "convert_Mlp(params)\n",
    "convert_embed(params)\n",
    "convert_pos_embed(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"params.txt\", \"w\") as file:\n",
    "    file.write(str(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[    2,     4,  2375, ...,    27,    10,   159],\n",
       "       [  123,    10,    27, ...,  1335,  3705,    42],\n",
       "       [17560,   138, 11541, ...,    46,    47, 12997],\n",
       "       ...,\n",
       "       [14413,  2771,     8, ...,    38,  9223,   378],\n",
       "       [18454,     7,    38, ..., 13874,    41, 14887],\n",
       "       [   13,     9,   724, ...,   121,    49,   386]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.apply({'params':params}, batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanodo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
